# DrupalGym Pipeline Configuration
project_name: "DrupalGym"
version: "1.0.0"
seed: 42

directories:
  raw: "raw"
  clean: "clean"
  sft: "sft"
  quality: "quality"
  dataset: "dataset"
  eval: "eval"
  models: "models"
  manifests: "manifests"
  sources: "sources"

sources:
  drupal_core:
    type: "git"
    url: "https://git.drupalcode.org/project/drupal.git"
    branch: "11.x"
  drupal_projects:
    type: "api"
    endpoint: "https://www.drupal.org/api-d7/node.json"
    filters:
      core_compatibility: "^11"
    user_agent: "DrupalGym/1.0"
    max_pages: 10
    limit: 1000
    sort: "changed"
    direction: "desc"
  discovery:
    max_pages: 20
    limit_per_page: 50
    min_last_changed_days: 365
    max_projects_after_filter: 200
  filters:
    require_drupal_core_constraint: "^11"
    require_php_constraint_min: "8.3"
    exclude_archived_or_security_only: true

acquisition:
  docs:
    max_pages_per_source: 300
    allowed_prefixes:
      drupal_docs:
        - "https://www.drupal.org/docs"
        - "https://www.drupal.org/docs/develop"
      drupal_api:
        - "https://api.drupal.org/api/drupal"
      symfony_docs:
        - "https://symfony.com/doc/7.0/"
      drupal_security:
        - "https://www.drupal.org/security"

normalization:
  dedup:
    exact_hash: true
    near_dup_enabled: true
    near_dup_method: "simhash_5gram"
    near_dup_threshold: 0.92

sft_generation:
  enable_symbol_kind_prompts: true
  enable_sdc_bundle_generation: true
  sdc_bundle_variants: 4
  include_extensions: [".php", ".module", ".inc", ".install", ".theme", ".yml", ".twig", ".md"]
  doc_source_allowlist_prefixes:
    - "docs/www_drupal_org/"
    - "docs/api_drupal_org/"
    - "repos/drupal_core/"
  doc_topic_denylist_terms:
    - "mcp"
    - "apidog"
    - "gitlab duo"
    - "workflow automation"
  doc_max_output_chars: 6000

quality:
  run_php_lint: true
  min_output_chars: 220
  max_output_chars: 30000
  reject_path_leakage_tokens: true
  path_leakage_tokens: ["repos/"]
  min_output_chars_by_type:
    code_reference: 220
    yaml_reference: 10
    twig_reference: 120
    sdc_reference: 120
    doc_summary: 180
  max_output_chars_by_type:
    code_reference: 24000
    yaml_reference: 8000
    twig_reference: 12000
    sdc_reference: 12000
    doc_summary: 12000
  max_numeric_line_streak: 12
  max_repeated_line_ratio: 0.31
  duplicate_output_mode: "normalized"
  reject_ambiguous_instruction_input: true
  max_outputs_per_instruction_input: 1
  require_non_empty_input_for_types:
    - "yaml_reference"
    - "twig_reference"
    - "sdc_reference"
  doc_source_allowlist_prefixes:
    - "docs/www_drupal_org/"
    - "docs/api_drupal_org/"
    - "repos/drupal_core/"
  doc_topic_denylist_terms:
    - "mcp"
    - "apidog"
    - "gitlab duo"
    - "workflow automation"

dataset:
  training_version: "v2"
  targets:
    train: 0.8
    valid: 0.1
    test: 0.1
  max_seq_len: 1024

dataset_refinement:
  input_version: "v1"
  output_version: "v2"
  seed: 42
  max_output_lines: 300
  max_output_chars: 6000
  max_source_share: 0.55
  deduplicate_normalized_output: true
  duplicate_resolution: "prefer_augmented_drop_retrieval"
  require_context_for_types:
    - "yaml_reference"
    - "twig_reference"
    - "code_reference"
    - "sdc_reference"
  chunk_instruction_mode: "metadata_only"
  weak_category_targets:
    attributes: 600
    di: 600
    sdc: 400
  weak_category_patterns:
    attributes:
      - "#\\[[A-Za-z_\\\\][A-Za-z0-9_\\\\]*"
    di:
      - "\\bContainerInterface\\b"
      - "public\\s+static\\s+function\\s+create\\s*\\("
      - "services\\.yml"
      - "logger\\.factory"
    routing:
      - "routing\\.yml"
      - "\\broute_name\\b"
      - "\\b_controller\\b"
      - "\\bpath:\\s*['\\\"/]"
    sdc:
      - "single\\s+directory\\s+component"
      - "component\\.yml"
      - "\\.component\\.yml"
      - "\\bsdc\\b"
  chunk_overlap_lines: 30
  target_test_ratio: 0.15
  exclude_test_sources_from_training_pool: true
  exclude_sources_prefixes:
    - "docs/symfony_com/"
  augmentation:
    enabled: true
    ratio: 0.75
    input_excerpt_lines: 60
    types:
      - "bugfix"
      - "refactor"
      - "write_from_spec"
      - "explain_and_implement"

training:
  test_run:
    max_seq_len: 2048
    padding_strategy: "dynamic"
    pad_to_multiple_of: 8
    max_steps: 100
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    gradient_accumulation_steps: 16
    learning_rate: 0.0002
    logging_steps: 5
    eval_strategy: "no"
    save_strategy: "no"
    fp16: false
    bf16: false
    gradient_checkpointing: true
    use_reentrant_gc: false
    group_by_length: false
    lora_r: 32
    lora_alpha: 64
    lora_dropout: 0.05
    lora_target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    bnb_4bit_compute_dtype: "float32"
    max_models: 1
    models:
      - name: "Qwen2.5-Coder-3B-Test"
        base_model: "Qwen/Qwen2.5-Coder-3B"
        training_type: "QLoRA"
  full_scale:
    max_seq_len: 2048
    padding_strategy: "dynamic"
    pad_to_multiple_of: 8
    num_train_epochs: 3
    max_steps: 150
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 1
    gradient_accumulation_steps: 16 # L40S-safe effective batch for current dataset volume
    learning_rate: 0.0001
    logging_steps: 1
    eval_strategy: "steps"
    eval_steps: 30
    save_strategy: "steps"
    save_steps: 50
    warmup_ratio: 0.1
    fp16: false
    bf16: true
    gradient_checkpointing: true
    use_reentrant_gc: false
    group_by_length: true
    lora_r: 64
    lora_alpha: 128
    lora_dropout: 0.05
    lora_target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    bnb_4bit_compute_dtype: "bfloat16"
    max_models: 1 # Focus on one model for the L40S full_scale session
    models:
      - name: "Qwen2.5-Coder-7B"
        base_model: "Qwen/Qwen2.5-Coder-7B"
        training_type: "QLoRA"

evaluation:
  seed: 42
  mode: "full_scale"
  max_new_tokens: 512
  device: "auto"
  max_models: 1
  run_php_lint: true
  run_phpcs: false
  max_code_checks_per_response: 3
  repetition_penalty: 1.1
  no_repeat_ngram_size: 4

export:
  formats: ["safetensors", "gguf"]
  quantization:
    gguf: ["Q4_K_M", "Q8_0"]
  merge_adapters: true
  tools:
    llama_cpp_dir: "/home/seutje/projects/llama.cpp"
    convert_hf_to_gguf: "/home/seutje/projects/llama.cpp/convert_hf_to_gguf.py"
    llama_quantize: "/home/seutje/projects/llama.cpp/build/bin/llama-quantize"

models:
  - name: "Qwen2.5-Coder-7B"
    base_model: "Qwen/Qwen2.5-Coder-7B"
    training_type: "QLoRA"
  - name: "Ministral-3-8B"
    base_model: "mistralai/Ministral-3-8B-Instruct-2410"
    training_type: "QLoRA"
