# DrupalGym Pipeline Configuration
project_name: "DrupalGym"
version: "1.0.0"
seed: 42

directories:
  raw: "raw"
  clean: "clean"
  sft: "sft"
  quality: "quality"
  dataset: "dataset"
  eval: "eval"
  models: "models"
  manifests: "manifests"
  sources: "sources"

sources:
  drupal_core:
    type: "git"
    url: "https://git.drupalcode.org/project/drupal.git"
    branch: "11.x"
  drupal_projects:
    type: "api"
    endpoint: "https://www.drupal.org/api-d7/node.json"
    filters:
      core_compatibility: "^11"
    user_agent: "DrupalGym/1.0"
    max_pages: 10
    limit: 1000
    sort: "changed"
    direction: "desc"

dataset:
  targets:
    train: 0.8
    valid: 0.1
    test: 0.1
  max_seq_len: 1024

training:
  test_run:
    max_seq_len: 2048
    max_steps: 100
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    gradient_accumulation_steps: 16
    learning_rate: 0.0002
    logging_steps: 5
    eval_strategy: "no"
    save_strategy: "no"
    fp16: false
    bf16: false
    gradient_checkpointing: true
    use_reentrant_gc: false
    group_by_length: false
    lora_r: 32
    lora_alpha: 64
    lora_dropout: 0.05
    lora_target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    bnb_4bit_compute_dtype: "float32"
    max_models: 1
    models:
      - name: "Qwen2.5-Coder-3B-Test"
        base_model: "Qwen/Qwen2.5-Coder-3B"
        training_type: "QLoRA"
  full_scale:
    max_seq_len: 8192
    num_train_epochs: 3
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    gradient_accumulation_steps: 16 # Increased for 20h target
    learning_rate: 0.0002
    logging_steps: 10
    eval_strategy: "steps"
    eval_steps: 500
    save_strategy: "steps"
    save_steps: 500
    fp16: false
    bf16: true
    gradient_checkpointing: true
    use_reentrant_gc: false
    group_by_length: true
    lora_r: 64
    lora_alpha: 128
    lora_dropout: 0.05
    lora_target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    bnb_4bit_compute_dtype: "bfloat16"
    max_models: 1 # Focus on one model for the 20h session
    models:
      - name: "Qwen2.5-Coder-7B"
        base_model: "Qwen/Qwen2.5-Coder-7B"
        training_type: "QLoRA"

export:
  formats: ["safetensors", "gguf"]
  quantization:
    gguf: ["Q4_K_M", "Q8_0"]
  merge_adapters: true
  tools:
    llama_cpp_dir:
    convert_hf_to_gguf:
    llama_quantize:

models:
  - name: "Qwen2.5-Coder-7B"
    base_model: "Qwen/Qwen2.5-Coder-7B"
    training_type: "QLoRA"
  - name: "Ministral-3-8B"
    base_model: "mistralai/Ministral-3-8B-Instruct-2410"
    training_type: "QLoRA"
