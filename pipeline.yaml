# DrupalGym Pipeline Configuration
project_name: "DrupalGym"
version: "1.0.0"
seed: 42

directories:
  raw: "raw"
  clean: "clean"
  sft: "sft"
  quality: "quality"
  dataset: "dataset"
  eval: "eval"
  models: "models"
  manifests: "manifests"
  sources: "sources"

sources:
  drupal_core:
    type: "git"
    url: "https://git.drupalcode.org/project/drupal.git"
    branch: "11.x"
  drupal_projects:
    type: "api"
    endpoint: "https://www.drupal.org/api-d7/node.json"
    filters:
      core_compatibility: "^11"
    user_agent: "DrupalGym/1.0"
    max_pages: 10
    limit: 50
    sort: "changed"
    direction: "desc"

dataset:
  targets:
    train: 0.8
    valid: 0.1
    test: 0.1
  max_seq_len: 1024

training:
  test_run:
    max_seq_len: 2048
    max_steps: 100
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    gradient_accumulation_steps: 16
    learning_rate: 0.0002
    logging_steps: 5
    eval_strategy: "no"
    save_strategy: "no"
    fp16: false
    bf16: false
    gradient_checkpointing: true
    use_reentrant_gc: false
    group_by_length: false
    lora_r: 32
    lora_alpha: 64
    lora_dropout: 0.05
    lora_target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    bnb_4bit_compute_dtype: "float32"
    max_models: 1
    models:
      - name: "Qwen2.5-Coder-3B-Test"
        base_model: "Qwen/Qwen2.5-Coder-3B"
        training_type: "QLoRA"

models:
  - name: "Qwen2.5-Coder-7B"
    base_model: "Qwen/Qwen2.5-Coder-7B"
    training_type: "QLoRA"
  - name: "Ministral-3-8B"
    base_model: "mistralai/Ministral-3-8B-Instruct-2410"
    training_type: "QLoRA"
