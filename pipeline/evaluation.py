import json
from pathlib import Path
from .logger import PipelineLogger

PROMPT_SUITE = [
    {
        "id": "block_attribute",
        "category": "attributes",
        "instruction": "Create a Drupal 11 Block plugin using PHP 8 attributes. The block ID should be 'gym_stats' and the label 'Gym Statistics'.",
    },
    {
        "id": "service_di",
        "category": "di",
        "instruction": "Define a Drupal 11 service in gym.services.yml and its class implementation using constructor injection for the 'logger.factory' service.",
    },
    {
        "id": "routing_sdc",
        "category": "sdc",
        "instruction": "How do I define a Single Directory Component (SDC) in Drupal 11? Show a basic directory structure and the component.yml file.",
    }
]

def run_evaluation_stage(config: dict, logger: PipelineLogger, root: Path):
    eval_dir = root / "eval"
    eval_dir.mkdir(parents=True, exist_ok=True)
    sample_outputs_dir = eval_dir / "sample_outputs"
    sample_outputs_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info("Starting evaluation suite...")
    
    results = []
    for prompt in PROMPT_SUITE:
        logger.info(f"Evaluating {prompt['id']}...")
        
        # Simulate model output for now
        output = f"// Simulated output for {prompt['id']}\n// In a real run, this would be generated by the model."
        
        results.append({
            "prompt": prompt,
            "generated_output": output,
            "metrics": {
                "length": len(output),
                "contains_attributes": "#[" in output,
                "contains_namespace": "namespace" in output
            }
        })
        
        # Save sample output
        with open(sample_outputs_dir / f"{prompt['id']}.txt", "w") as f:
            f.write(output)

    metrics_path = eval_dir / "metrics.json"
    with open(metrics_path, "w") as f:
        json.dump(results, f, indent=2)
        
    logger.info(f"Evaluation complete. Results saved to {metrics_path}")
    return 0
